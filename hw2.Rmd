---
title: "hw2"
author: "Kee-Young Shin"
date: "September 29, 2018"
output: github_document
---

## Problem 1
### Create subdirectory
```{r}
library(tidyverse)


dir.create("./hw2data")

```

```{r}

transit_data = read.csv("./hw2data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv") %>%
  janitor::clean_names() %>% # clean name of data
  select(line:route11, entry, entrance_type, vending, ada) %>% # filter columns 
  mutate(entry = ifelse(entry == "YES", TRUE, FALSE))  # convert character into logical

# The dataset contains information regarding NYC subway stations including, the line name,
# its location, its various routes, its entrance type, whether it has vending, whether it
# allows entry, and whether or not it is ADA compliant. I cleaned the dataset by converting # the names into snake case and taking out superfluous columns such as the coordinates for 
# the entrance. The new data set contained 1868 rows and 19 columns. 

dim(transit_data) # show rows and columns of data

transit_data
 

transit_data %>%
  distinct(line, station_name)  # Count number of distinct stations
 
  
distinct(transit_data, line, station_name, route1, route2, route3, route4, route5, route6, route7, route8, route9, route10, route11)

sum(transit_data$ada == "TRUE") # Count how many stations are ADA compliant

transit_data %>%
	filter(vending == "NO") %>%  # show only entrances without vending 
  summarize(total = n(), yes = sum(entry == "TRUE")) %>% # find number of allowed entries
  mutate(prop = yes / total) # find proportion 

	





```

## Problem 2

```{r}
library(readxl)

wheel_data = read_excel("./hw2data/HealthyHarborWaterWheelTotals2018-7-28.xlsx", 
	  sheet = 1, range = cell_cols("A:N")) %>%
	  janitor::clean_names() %>% # import data and clean names
    filter(!is.na(dumpster)) %>% # filter out non dumpster specific data 
    mutate(sports_balls = as.integer(round(sports_balls))) # rounding sports balls 
   
wheel_data

prcp_2018 = read_excel("./hw2data/HealthyHarborWaterWheelTotals2018-7-28.xlsx",
                       sheet = "2018 Precipitation", skip = 1 ) %>%  # import data
  janitor::clean_names() %>%  # clean name
  na.omit() %>% # remove na  
  mutate(year = 2018) # put in year column

prcp_2018

prcp_2017 = read_excel("./hw2data/HealthyHarborWaterWheelTotals2018-7-28.xlsx",
                       sheet = "2017 Precipitation", skip = 1 ) %>%  # import data 
  janitor::clean_names() %>%   # clean name
  na.omit() %>%  # remove na 
  mutate(year = 2017)  # put in year column 

prcp_2017

 
combined_prcp = bind_rows(prcp_2018, prcp_2017)  # combine precipitation data 
    

month_df = data.frame(
  month = 1:12, name = month.name, stringsAsFactors = FALSE) # created month df
month_df
  
combined_prcp = inner_join(combined_prcp, month_df, by = "month") # added month df 

combined_prcp

combined_prcp %>%
  filter(year == 2018) %>%
  summarize(total = sum(total))
combined_prcp %>%
  filter(year == 2017) %>%
  summarize(total = sum(total))
wheel_data %>%
  filter(year == 2017) %>%
  summarize(median = median(sports_balls))


filter(wheel_data, year == 2018) %>%
  summarize(sum(weight_tons))

```
There were `r nrow(combined_prcp)` and `r nrow(wheel_data)` observations for the precipitation data set and the trash wheel dataset, respectively.The total precipitation for 2017 was `r sum(prcp_2017$total)` and `r sum(prcp_2018$total)` for 2018. The median for precipitation for 2017 and 2018 were `r median(prcp_2017$total)` and `r median(prcp_2018$total)`, respectively. Some key variables for the trash dataset include the total trash weight in 2018 -- `r filter(wheel_data, year == 2018) %>% summarize(sum(weight_tons))` tons, median number of sports balls in 2017 -- `r wheel_data %>% filter(year == 2017) %>% summarize(median = median(sports_balls))` balls, and the mean of plastic bottles thrown out in 2018 -- `r filter(wheel_data, year == 2018) %>% summarize(mean(plastic_bottles))` bottles. 


## Problem 3
```{r}
devtools::install_github("p8105/p8105.datasets")

library(p8105.datasets)

data("brfss_smart2010") # import data set 


filtered_brfss = brfss_smart2010 %>%
  janitor::clean_names() %>% # clean names 
  filter(topic == "Overall Health") %>%
  select(-class, -topic, -question, -sample_size, -(confidence_limit_low:geo_location)) %>%
  spread(key = "response", value = "data_value") %>%  # making rows into columns
  janitor::clean_names() %>% # clean new columns 
  mutate(proportion_yv = excellent + very_good) # create proportion column

filtered_brfss  

distinct(filtered_brfss, locationdesc) # distinct locations
# There are 404 distinct locations represented in this data set.
distinct(filtered_brfss, locationabbr) # distinct states
# All states are represented in addition to D.C.

filtered_brfss %>%
  group_by(locationabbr) %>%
  summarize(n = n()) %>% # take count of number of state
  arrange(-n) # put the count in descending order to show which state was observed most 
# New Jersey was observed most with 146 counts. 


filtered_brfss %>%
  filter(year == 2002) %>% # filter by year 2002
  summarize(median = median(excellent, na.rm = TRUE)) # show median for excellent

filtered_brfss %>%
  filter(year == 2002) %>% # filter by year 
  ggplot(aes(x = excellent)) + geom_histogram() # create histogram


filtered_brfss %>%
  filter(locationdesc %in% 
           c("NY - New York County", "NY - Queens County")) %>% # filter by location
  ggplot(aes(x = year, y = proportion_yv, color = locationdesc)) +
  geom_point() # create scatter plot



```


